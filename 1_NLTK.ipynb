{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kOQ8HqLucpj6",
        "b8Rrj78vcYEE",
        "d16Qhtrtccef",
        "AfXHVCrrcfDn",
        "8hK_jUI_cgcy",
        "sGM_M-QHch5T",
        "2XOJPl4dcjwA",
        "ySRcdmVTclu6"
      ],
      "authorship_tag": "ABX9TyM9gPHK5DfiQHVEAne1eBqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ffneiva/bootcamp-llm/blob/main/1_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importação e download de pacotes"
      ],
      "metadata": {
        "id": "kOQ8HqLucpj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag, pos_tag_sents\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIvE68Y2crLH",
        "outputId": "10fe6339-146a-4f24-9c8f-3afe2ccc8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQn-eNR3eOLj",
        "outputId": "37bd7472-02c1-422c-82fb-20331b5ab6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [*] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [*] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [*] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [*] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [*] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [*] words............... Word Lists\n",
            "Hit Enter to continue: \n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [P] all-corpora......... All the corpora\n",
            "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [P] all................. All packages\n",
            "  [P] book................ Everything used in the NLTK Book\n",
            "  [P] popular............. Popular packages\n",
            "  [P] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages; [P] marks partially installed collections)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produção de tokens"
      ],
      "metadata": {
        "id": "b8Rrj78vcYEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer existiram. Mas sem ela não temos a lugar algum.'\n",
        "sentences = sent_tokenize(text)\n",
        "len(sentences), sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4fwfkz7elQ8",
        "outputId": "da6b69d4-fc65-4897-b38f-7e2c7932731d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " ['Nós somos feitos de poeira de estrelas.',\n",
              "  'Nós somos uma maneira de o cosmos se autoconhecer.',\n",
              "  'A imaginação nos leva a mundos que nunca sequer existiram.',\n",
              "  'Mas sem ela não temos a lugar algum.'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFACNesdcPTh",
        "outputId": "c89b2b47-8f09-4b13-b483-aedd75a3c612"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38,\n",
              " ['Nós',\n",
              "  'somos',\n",
              "  'feitos',\n",
              "  'de',\n",
              "  'poeira',\n",
              "  'de',\n",
              "  'estrelas',\n",
              "  '.',\n",
              "  'Nós',\n",
              "  'somos',\n",
              "  'uma',\n",
              "  'maneira',\n",
              "  'de',\n",
              "  'o',\n",
              "  'cosmos',\n",
              "  'se',\n",
              "  'autoconhecer',\n",
              "  '.',\n",
              "  'A',\n",
              "  'imaginação',\n",
              "  'nos',\n",
              "  'leva',\n",
              "  'a',\n",
              "  'mundos',\n",
              "  'que',\n",
              "  'nunca',\n",
              "  'sequer',\n",
              "  'existiram',\n",
              "  '.',\n",
              "  'Mas',\n",
              "  'sem',\n",
              "  'ela',\n",
              "  'não',\n",
              "  'temos',\n",
              "  'a',\n",
              "  'lugar',\n",
              "  'algum',\n",
              "  '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokens = word_tokenize(text, language='portuguese')\n",
        "len(tokens), tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words e pontuação"
      ],
      "metadata": {
        "id": "d16Qhtrtccef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stops = stopwords.words('portuguese')\n",
        "len(stops), stops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1rvzF9CcesF",
        "outputId": "f6714407-f191-4c3a-8735-bea8f4e0ebc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207,\n",
              " ['a',\n",
              "  'à',\n",
              "  'ao',\n",
              "  'aos',\n",
              "  'aquela',\n",
              "  'aquelas',\n",
              "  'aquele',\n",
              "  'aqueles',\n",
              "  'aquilo',\n",
              "  'as',\n",
              "  'às',\n",
              "  'até',\n",
              "  'com',\n",
              "  'como',\n",
              "  'da',\n",
              "  'das',\n",
              "  'de',\n",
              "  'dela',\n",
              "  'delas',\n",
              "  'dele',\n",
              "  'deles',\n",
              "  'depois',\n",
              "  'do',\n",
              "  'dos',\n",
              "  'e',\n",
              "  'é',\n",
              "  'ela',\n",
              "  'elas',\n",
              "  'ele',\n",
              "  'eles',\n",
              "  'em',\n",
              "  'entre',\n",
              "  'era',\n",
              "  'eram',\n",
              "  'éramos',\n",
              "  'essa',\n",
              "  'essas',\n",
              "  'esse',\n",
              "  'esses',\n",
              "  'esta',\n",
              "  'está',\n",
              "  'estamos',\n",
              "  'estão',\n",
              "  'estar',\n",
              "  'estas',\n",
              "  'estava',\n",
              "  'estavam',\n",
              "  'estávamos',\n",
              "  'este',\n",
              "  'esteja',\n",
              "  'estejam',\n",
              "  'estejamos',\n",
              "  'estes',\n",
              "  'esteve',\n",
              "  'estive',\n",
              "  'estivemos',\n",
              "  'estiver',\n",
              "  'estivera',\n",
              "  'estiveram',\n",
              "  'estivéramos',\n",
              "  'estiverem',\n",
              "  'estivermos',\n",
              "  'estivesse',\n",
              "  'estivessem',\n",
              "  'estivéssemos',\n",
              "  'estou',\n",
              "  'eu',\n",
              "  'foi',\n",
              "  'fomos',\n",
              "  'for',\n",
              "  'fora',\n",
              "  'foram',\n",
              "  'fôramos',\n",
              "  'forem',\n",
              "  'formos',\n",
              "  'fosse',\n",
              "  'fossem',\n",
              "  'fôssemos',\n",
              "  'fui',\n",
              "  'há',\n",
              "  'haja',\n",
              "  'hajam',\n",
              "  'hajamos',\n",
              "  'hão',\n",
              "  'havemos',\n",
              "  'haver',\n",
              "  'hei',\n",
              "  'houve',\n",
              "  'houvemos',\n",
              "  'houver',\n",
              "  'houvera',\n",
              "  'houverá',\n",
              "  'houveram',\n",
              "  'houvéramos',\n",
              "  'houverão',\n",
              "  'houverei',\n",
              "  'houverem',\n",
              "  'houveremos',\n",
              "  'houveria',\n",
              "  'houveriam',\n",
              "  'houveríamos',\n",
              "  'houvermos',\n",
              "  'houvesse',\n",
              "  'houvessem',\n",
              "  'houvéssemos',\n",
              "  'isso',\n",
              "  'isto',\n",
              "  'já',\n",
              "  'lhe',\n",
              "  'lhes',\n",
              "  'mais',\n",
              "  'mas',\n",
              "  'me',\n",
              "  'mesmo',\n",
              "  'meu',\n",
              "  'meus',\n",
              "  'minha',\n",
              "  'minhas',\n",
              "  'muito',\n",
              "  'na',\n",
              "  'não',\n",
              "  'nas',\n",
              "  'nem',\n",
              "  'no',\n",
              "  'nos',\n",
              "  'nós',\n",
              "  'nossa',\n",
              "  'nossas',\n",
              "  'nosso',\n",
              "  'nossos',\n",
              "  'num',\n",
              "  'numa',\n",
              "  'o',\n",
              "  'os',\n",
              "  'ou',\n",
              "  'para',\n",
              "  'pela',\n",
              "  'pelas',\n",
              "  'pelo',\n",
              "  'pelos',\n",
              "  'por',\n",
              "  'qual',\n",
              "  'quando',\n",
              "  'que',\n",
              "  'quem',\n",
              "  'são',\n",
              "  'se',\n",
              "  'seja',\n",
              "  'sejam',\n",
              "  'sejamos',\n",
              "  'sem',\n",
              "  'ser',\n",
              "  'será',\n",
              "  'serão',\n",
              "  'serei',\n",
              "  'seremos',\n",
              "  'seria',\n",
              "  'seriam',\n",
              "  'seríamos',\n",
              "  'seu',\n",
              "  'seus',\n",
              "  'só',\n",
              "  'somos',\n",
              "  'sou',\n",
              "  'sua',\n",
              "  'suas',\n",
              "  'também',\n",
              "  'te',\n",
              "  'tem',\n",
              "  'tém',\n",
              "  'temos',\n",
              "  'tenha',\n",
              "  'tenham',\n",
              "  'tenhamos',\n",
              "  'tenho',\n",
              "  'terá',\n",
              "  'terão',\n",
              "  'terei',\n",
              "  'teremos',\n",
              "  'teria',\n",
              "  'teriam',\n",
              "  'teríamos',\n",
              "  'teu',\n",
              "  'teus',\n",
              "  'teve',\n",
              "  'tinha',\n",
              "  'tinham',\n",
              "  'tínhamos',\n",
              "  'tive',\n",
              "  'tivemos',\n",
              "  'tiver',\n",
              "  'tivera',\n",
              "  'tiveram',\n",
              "  'tivéramos',\n",
              "  'tiverem',\n",
              "  'tivermos',\n",
              "  'tivesse',\n",
              "  'tivessem',\n",
              "  'tivéssemos',\n",
              "  'tu',\n",
              "  'tua',\n",
              "  'tuas',\n",
              "  'um',\n",
              "  'uma',\n",
              "  'você',\n",
              "  'vocês',\n",
              "  'vos'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_stops = [token for token in tokens if not token in stops]\n",
        "len(no_stops), no_stops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4iISZN9gVcW",
        "outputId": "d2ec52e8-0f6c-4c2d-89ce-c4a37aae4925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22,\n",
              " ['Nós',\n",
              "  'feitos',\n",
              "  'poeira',\n",
              "  'estrelas',\n",
              "  '.',\n",
              "  'Nós',\n",
              "  'maneira',\n",
              "  'cosmos',\n",
              "  'autoconhecer',\n",
              "  '.',\n",
              "  'A',\n",
              "  'imaginação',\n",
              "  'leva',\n",
              "  'mundos',\n",
              "  'nunca',\n",
              "  'sequer',\n",
              "  'existiram',\n",
              "  '.',\n",
              "  'Mas',\n",
              "  'lugar',\n",
              "  'algum',\n",
              "  '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(string.punctuation), string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BWUeeBQgmbK",
        "outputId": "2176c99a-00af-4421-ff42-64a9d7f9c3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_stops_or_puncs = [token for token in no_stops if not token in string.punctuation]\n",
        "len(no_stops_or_puncs), no_stops_or_puncs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD9gaiHug8tH",
        "outputId": "9cd20a75-2b13-450e-eb48-cb575b9c5827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18,\n",
              " ['Nós',\n",
              "  'feitos',\n",
              "  'poeira',\n",
              "  'estrelas',\n",
              "  'Nós',\n",
              "  'maneira',\n",
              "  'cosmos',\n",
              "  'autoconhecer',\n",
              "  'A',\n",
              "  'imaginação',\n",
              "  'leva',\n",
              "  'mundos',\n",
              "  'nunca',\n",
              "  'sequer',\n",
              "  'existiram',\n",
              "  'Mas',\n",
              "  'lugar',\n",
              "  'algum'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "AfXHVCrrcfDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq = nltk.FreqDist(no_stops_or_puncs)\n",
        "freq, freq.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F68mtEs6cf7N",
        "outputId": "d53eaaa8-79a9-441f-e8c2-7bd4362277a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(FreqDist({'Nós': 2, 'feitos': 1, 'poeira': 1, 'estrelas': 1, 'maneira': 1, 'cosmos': 1, 'autoconhecer': 1, 'A': 1, 'imaginação': 1, 'leva': 1, ...}),\n",
              " [('Nós', 2),\n",
              "  ('feitos', 1),\n",
              "  ('poeira', 1),\n",
              "  ('estrelas', 1),\n",
              "  ('maneira', 1),\n",
              "  ('cosmos', 1),\n",
              "  ('autoconhecer', 1),\n",
              "  ('A', 1),\n",
              "  ('imaginação', 1),\n",
              "  ('leva', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "8hK_jUI_cgcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer1 = PorterStemmer()\n",
        "stemmed1 = [stemmer1.stem(token) for token in no_stops_or_puncs]\n",
        "stemmed1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhND2E_bchhB",
        "outputId": "8a38b51e-9fc1-496e-c9e1-ad0017c98049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nó',\n",
              " 'feito',\n",
              " 'poeira',\n",
              " 'estrela',\n",
              " 'nó',\n",
              " 'maneira',\n",
              " 'cosmo',\n",
              " 'autoconhec',\n",
              " 'a',\n",
              " 'imaginação',\n",
              " 'leva',\n",
              " 'mundo',\n",
              " 'nunca',\n",
              " 'sequer',\n",
              " 'existiram',\n",
              " 'ma',\n",
              " 'lugar',\n",
              " 'algum']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer2 = SnowballStemmer('portuguese')\n",
        "stemmed2 = [stemmer2.stem(token) for token in no_stops_or_puncs]\n",
        "stemmed2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu2fBmw5iWsr",
        "outputId": "00b5cf31-4fef-4f6c-a8f1-7f0a29f924d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nós',\n",
              " 'feit',\n",
              " 'poeir',\n",
              " 'estrel',\n",
              " 'nós',\n",
              " 'maneir',\n",
              " 'cosm',\n",
              " 'autoconhec',\n",
              " 'a',\n",
              " 'imagin',\n",
              " 'lev',\n",
              " 'mund',\n",
              " 'nunc',\n",
              " 'sequ',\n",
              " 'exist',\n",
              " 'mas',\n",
              " 'lug',\n",
              " 'algum']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer3 = LancasterStemmer()\n",
        "stemmed3 = [stemmer3.stem(token) for token in no_stops_or_puncs]\n",
        "stemmed3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdQifyiSihuD",
        "outputId": "f48e38b1-0c08-410a-f259-ee60ef49f5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nós',\n",
              " 'feito',\n",
              " 'poeir',\n",
              " 'estrela',\n",
              " 'nós',\n",
              " 'maneir',\n",
              " 'cosmo',\n",
              " 'autoconhec',\n",
              " 'a',\n",
              " 'imaginação',\n",
              " 'lev',\n",
              " 'mundo',\n",
              " 'nunc',\n",
              " 'sequ',\n",
              " 'existiram',\n",
              " 'mas',\n",
              " 'lug',\n",
              " 'alg']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(no_stops_or_puncs):\n",
        "    print(f'{token} - {stemmed1[i]} - {stemmed2[i]} - {stemmed3[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YABCcwD-ij0-",
        "outputId": "96c54b06-361e-486f-e57a-215e5b2d1f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nós - nó - nós - nós\n",
            "feitos - feito - feit - feito\n",
            "poeira - poeira - poeir - poeir\n",
            "estrelas - estrela - estrel - estrela\n",
            "Nós - nó - nós - nós\n",
            "maneira - maneira - maneir - maneir\n",
            "cosmos - cosmo - cosm - cosmo\n",
            "autoconhecer - autoconhec - autoconhec - autoconhec\n",
            "A - a - a - a\n",
            "imaginação - imaginação - imagin - imaginação\n",
            "leva - leva - lev - lev\n",
            "mundos - mundo - mund - mundo\n",
            "nunca - nunca - nunc - nunc\n",
            "sequer - sequer - sequ - sequ\n",
            "existiram - existiram - exist - existiram\n",
            "Mas - ma - mas - mas\n",
            "lugar - lugar - lug - lug\n",
            "algum - algum - algum - alg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pós-tagging"
      ],
      "metadata": {
        "id": "sGM_M-QHch5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvS-1qsQcjNk",
        "outputId": "a2046795-1f17-4a6a-983f-5c553772fc3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = [word_tokenize(word) for word in sentences]\n",
        "pos_tag_sents(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ4vPPbYjbgC",
        "outputId": "d5fc090e-de77-42d8-ec54-ecf75e69b403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Nós', 'NNP'),\n",
              "  ('somos', 'VBD'),\n",
              "  ('feitos', 'NNS'),\n",
              "  ('de', 'FW'),\n",
              "  ('poeira', 'FW'),\n",
              "  ('de', 'FW'),\n",
              "  ('estrelas', 'FW'),\n",
              "  ('.', '.')],\n",
              " [('Nós', 'NNP'),\n",
              "  ('somos', 'NN'),\n",
              "  ('uma', 'JJ'),\n",
              "  ('maneira', 'NN'),\n",
              "  ('de', 'IN'),\n",
              "  ('o', 'FW'),\n",
              "  ('cosmos', 'NNS'),\n",
              "  ('se', 'JJ'),\n",
              "  ('autoconhecer', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('A', 'DT'),\n",
              "  ('imaginação', 'JJ'),\n",
              "  ('nos', 'JJ'),\n",
              "  ('leva', 'NN'),\n",
              "  ('a', 'DT'),\n",
              "  ('mundos', 'NN'),\n",
              "  ('que', 'NN'),\n",
              "  ('nunca', 'JJ'),\n",
              "  ('sequer', 'NN'),\n",
              "  ('existiram', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('Mas', 'NNP'),\n",
              "  ('sem', 'NN'),\n",
              "  ('ela', 'NN'),\n",
              "  ('não', 'JJ'),\n",
              "  ('temos', 'VBZ'),\n",
              "  ('a', 'DT'),\n",
              "  ('lugar', 'NN'),\n",
              "  ('algum', 'NN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "2XOJPl4dcjwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in no_stops_or_puncs]\n",
        "lemmatized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AevX-4tclOI",
        "outputId": "324fd988-ed7d-435f-b370-c6f731062432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nós',\n",
              " 'feitos',\n",
              " 'poeira',\n",
              " 'estrelas',\n",
              " 'Nós',\n",
              " 'maneira',\n",
              " 'cosmos',\n",
              " 'autoconhecer',\n",
              " 'A',\n",
              " 'imaginação',\n",
              " 'lev',\n",
              " 'mundos',\n",
              " 'nunca',\n",
              " 'sequer',\n",
              " 'existiram',\n",
              " 'Mas',\n",
              " 'lugar',\n",
              " 'algum']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entidades nomeadas"
      ],
      "metadata": {
        "id": "ySRcdmVTclu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ne_text = 'Barack Obama não foi um presidente do Brasil'\n",
        "tokens = word_tokenize(ne_text)\n",
        "ne_tags = nltk.ne_chunk(pos_tag(tokens))\n",
        "ne_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "NKL2TP_4cm2s",
        "outputId": "3fb6c584-7b44-4808-b56e-9f5d07f950f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'svgling'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msvgling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('PERSON', [('Obama', 'NNP')]), ('não', 'MD'), ('foi', 'VB'), ('um', 'JJ'), ('presidente', 'NN'), ('do', 'VBP'), Tree('PERSON', [('Brasil', 'NNP')])])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}